{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaReNet - CapsNet based regression network\n",
    "\n",
    "## Application: Facial keypoint Detection using CaReNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook contains the implementations details of the CaReNet in the NCVPRIPG <a href=\"https://github.com/abhinay-b/Deep-Learning\">publication</a>. \n",
    "\n",
    "We propose a capsule based regression network (CaReNet),a framework that is based on capsule networks (CapsNet), rather than on the conventional convolutional neural networks (CNNs) to determine estimates of continuous variables. The core principles of CaReNet remain that of routing-by-agreement and translation equivariance proposed inthe CapsNet architecture for classification problems. However, unlike the CapsNet architecture, the final layer capsules in CaReNet architecture capture the number of values to regress in their dimensionality. An output vector returned by each of these capsules contains all the regressed values while the corresponding activity vector, determined by squashing an output vector, captures the likelihood of the regressed values being present in the corresponding capsule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Encoder:\n",
    "<img src=\"../Images/2DCaReNet.jpg\" width=\"1200\" />\n",
    "\n",
    "The architecture of our 2D capsule-based regression network (2D CaReNet) for facial key point detection has an encoder structure with the following layers similar to that in <a href= \"https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf\"> CapsNet</a>:\n",
    "\n",
    "- **Convolutional layer**: This layer has 256 convolution kernels of size 9 $\\times$ 9 with a stride of 1 and ReLU activation unit.\n",
    "- **PrimaryCapsule layer**: This layer is made up of 32 capsules and each capsule has 8 convolutional units with each having a kernel of size 16 $ \\times $ 16 and stride of 12.\n",
    "- **RoutingCapsule layer**: This layer is made up of ten 30D capsules. Each of the capsules obtains the input from all the capsules in PrimaryCapsules. Further, each capsule outputs a vector that gives the corresponding activity vector of the capsule when squashed using the squash function given in equation. As the length of an activity vector determines the probability that an entity exists at the corresponding capsule, we use the squashed vectors or the activity vectors to determine the most probable capsule to be routed dynamically. The non-squashed values are not used for any other purpose in this layer.\n",
    "- **DetectionCapsule layer**: This layer is also made up of ten 30D capsules and the squashed vectors are used to dynamically determine the capsule to be routed to. Further, in this layer, the \\textbf{non-squashed vector with the highest length} in the corresponding activity vector is considered as the regressed output vector that captures the expected keypoints in its dimensions. Note that both the non-squashed values and the squashed values are used here. The squashed values give the probability of routing to a particular capsule (as proposed in <a href= \"https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf\"> CapsNet</a>) while non-squashed values give the expected key points regressed by that capsule(proposed in this work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder:\n",
    "The decoder structure is very similar to the one in <a href= \"https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf\"> CapsNet</a>:\n",
    "\n",
    "<img src=\"../Images/decoder.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from capsnet_utils import *\n",
    "import torch.optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torchvision.utils as vutils\n",
    "from pandas.io.parsers import read_csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squash function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_squash(s, dim = 2):\n",
    "    \n",
    "    s_norm = torch.norm(s, p = 2, dim = dim, keepdim = True)\n",
    "    s_norm_sqr = s_norm ** 2     \n",
    "    scalar = s_norm_sqr / (1. + s_norm_sqr)\n",
    "    v = scalar * (s / s_norm)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_activity(W, u):\n",
    "    \n",
    "    num_capsules_higher = W.size(2)\n",
    "    \n",
    "    # size of W: batch_size x # capsules in lower_layer x # capsules in higher_layer \n",
    "    #                       x capsule_dim in higher layer x capsule_dim in lower layer\n",
    "    # size of u: batch_size x # capsules in lower_layer  \n",
    "    #                       x capsule_dim in lower layer x 1\n",
    "    # if we stack u along dim 2 # capsules in higher_layer times, size of u will become\n",
    "    #           batch_size x # capsules in lower_layer x # capsules in higher_layer\n",
    "    #                      x capsule_dim in lower layer x 1\n",
    "    # so we can directly invoke torch.matmul to do the multiplication of two tensors and get u_hat whose size is\n",
    "    #           batch_size x # capsules in lower_layer x # capsules in higher_layer \n",
    "    #                      x capsule_dim in higher layer x 1\n",
    "    \n",
    "    u = torch.stack([u]*num_capsules_higher, dim = 2)\n",
    "    u_hat = torch.matmul(W, u) \n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic routing algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_routing(u_hat, r):\n",
    "    \"\"\"\n",
    "    Ip:\n",
    "        u_hat: batch_size x #caps in layer l x #caps in layer l+1 x cap_dim in layer l+1 x 1\n",
    "        r: number of routing iterations    \n",
    "    Return:\n",
    "        v: activity vectors in layer l+1.\n",
    "           size of v is batch_size x #caps in layer l+1 x cap_dim in layer l+1 x 1\n",
    "    \"\"\"\n",
    "    batch_size = u_hat.size(0)\n",
    "    b = Variable(torch.zeros(1, u_hat.size(1), u_hat.size(2), 1))\n",
    "    if torch.cuda.is_available():\n",
    "        b = b.cuda()\n",
    "    for riter in range(r):\n",
    "        c = softmax(b, dim = 1) #see eqn(3)\n",
    "        c = torch.cat([c] * batch_size, dim = 0).unsqueeze(4) # to take advantage of python \n",
    "                                                                # broadcasting to do step 5 in Fig 2\n",
    "        s = torch.sum(c * u_hat, dim = 1) # step 5\n",
    "        v = my_squash(s, dim = 2) # step 6\n",
    "        v_temp = torch.stack([v] * u_hat.size(1), dim = 1) # required for vectorizing step 7 in Fig 2\n",
    "        b = b + torch.matmul(u_hat.transpose(3, 4), v_temp).squeeze(4).mean(dim = 0, keepdim = True)\n",
    "                                                           # step 7\n",
    "    return v,s  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Capsule layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # create primary capsule layer\n",
    "        #primary capsule has 32 channels of (8D) capsules and each capsule has 8 convolutional units\n",
    "        self.conv_units = nn.ModuleList([nn.Conv2d(256, 32, kernel_size = (16, 16),\n",
    "                                                          stride = (12, 12)) for i in range(8)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # forward prop through primary capsule layer          \n",
    "        outputs = [m(x) for i, m in enumerate(self.conv_units)] # a list of 8 outputs\n",
    "        outputs = torch.stack(outputs, dim = 4) # stack all of them along dim 4\n",
    "        #assert capsules.size() == [x.size(0), 32, 6, 6, 8]\n",
    "        s = outputs.view(x.size(0), -1, 8).unsqueeze(dim = 3) \n",
    "                                            # reshape to size batch_size x 1152 x 8 x 1\n",
    "        v = my_squash(s, 2) # squash along dim 2           \n",
    "        #assert capsules.size() == [x.size(0), 1152, 8, 1]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing Capsule layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RoutingCapsuleLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, lowerCapsCount, higherCapsCount, higherCapsDim, lowerCapsDim, iterations):\n",
    "        \n",
    "        super().__init__()\n",
    "        # digits capsule; so create W as a learnable set of parameters initialized randomly\n",
    "        self.W = nn.Parameter(torch.randn(1, lowerCapsCount, higherCapsCount, higherCapsDim, lowerCapsDim)) # 1st dim is 1 since right now we dont\n",
    "                                                               # know batch_size\n",
    "        self.routing_iterations = iterations\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #forward prop through digits capsule by prediciting activity and routing\n",
    "        u_hat = predict_activity(self.W, x)\n",
    "        v, s = dynamic_routing(u_hat, self.routing_iterations)\n",
    "        return v, s   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask(x):\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    #masked_x = Variable(torch.zeros(x.size()))\n",
    "    #if torch.cuda.is_available():\n",
    "     #   masked_x = masked_x.cuda()\n",
    "    v_norm = torch.norm(x, p = 2, dim = 2)   \n",
    "    _, max_index = v_norm.max(dim = 1)\n",
    "    #max_index = max_index.numpy().squeeze().list()\n",
    "    #masked_x[range(batch_size)] = x[range(batch_size), max_index]\n",
    "    max_index = max_index.data    \n",
    "    masked_v = []\n",
    "    max_v_indices = []\n",
    "    for batch_ix in range(batch_size):\n",
    "        # Batch sample\n",
    "        sample = x[batch_ix]\n",
    "        # Masks out the other capsules in this sample.\n",
    "        v = Variable(torch.zeros(sample.size()))\n",
    "        if torch.cuda.is_available():\n",
    "            v = v.cuda()\n",
    "        # Get the maximum capsule index from this batch sample.\n",
    "        max_caps_index = max_index[batch_ix]\n",
    "        v[max_caps_index] = sample[max_caps_index]\n",
    "        max_v_indices.append(max_caps_index)\n",
    "        masked_v.append(v) # append v to masked_v\n",
    "\n",
    "    # Concatenates sequence of masked capsules tensors along the batch dimension.\n",
    "    masked = torch.stack(masked_v, dim=0)\n",
    "    max_i = torch.stack(max_v_indices, dim = 0)\n",
    "    #print(masked.size())\n",
    "    return masked, max_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(s, indices):\n",
    "    batch_size = s.size(0)\n",
    "    max_s_values = []\n",
    "    for batch_ix in range(batch_size):\n",
    "        sample = s[batch_ix]\n",
    "        max_s_values.append(sample[indices[batch_ix]])\n",
    "    extracted = torch.stack(max_s_values, dim = 0)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, use_reconst= True):\n",
    "        \n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(1, 256, kernel_size = (9, 9), stride = (1, 1))        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.primary_capsule = PrimaryCapsuleLayer()\n",
    "        self.detection_capsule1 = RoutingCapsuleLayer(1568, 10, 30, 8, 3)        \n",
    "        self.detection_capsule2 = RoutingCapsuleLayer(10, 10, 30, 30, 3)  \n",
    "        self.use_reconst= use_reconst\n",
    "        \n",
    "        # for margin loss\n",
    "        self.loss_criterion = nn.MSELoss(size_average = True)\n",
    "        \n",
    "        if (self.use_reconst):\n",
    "            self.fcReconst1= nn.Linear(30 * 10,1024)\n",
    "            self.reluReconst1= nn.ReLU()\n",
    "            self.fcReconst2= nn.Linear(1024,4096)\n",
    "            self.reluReconst2= nn.ReLU()\n",
    "            self.fcReconst3= nn.Linear(4096, 9216)\n",
    "            self.sigmoid= nn.Sigmoid()\n",
    "               \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)        \n",
    "        x = self.relu1(x)\n",
    "        x = self.primary_capsule(x)        \n",
    "        x, _ = self.detection_capsule1(x) \n",
    "        #Here's our innovation; return squashed and non-sqauashed values\n",
    "        x, s = self.detection_capsule2(x)\n",
    "        return x,s\n",
    "    \n",
    "    def margin_loss(self, model_output, ground_truth):\n",
    "        model_output = model_output.view(-1,30,1)\n",
    "        return self.loss_criterion(model_output, ground_truth)\n",
    "        \n",
    "    def reconstruction_loss(self, reconstructed, original, size_average= True):\n",
    "        #square\n",
    "        rec_loss = (reconstructed - original.view(reconstructed.size(0), -1)) ** 2\n",
    "        #sum\n",
    "        rec_loss = torch.sum(rec_loss, dim = 1)\n",
    "        #mean\n",
    "        if (size_average):\n",
    "            rec_loss= rec_loss.mean()\n",
    "        return rec_loss\n",
    "    \n",
    "    def model_loss(self, x,s, ground_truth, input_image, size_average= True):\n",
    "        \n",
    "        #output_norm = torch.norm(model_output, p = 2, dim = 2) \n",
    "        masked, indices = mask(x)\n",
    "        extracted = extract(s, indices)\n",
    "        # calculate margin loss\n",
    "        loss = self.margin_loss(extracted, ground_truth)\n",
    "        # if use reconstruction, then reconstruct and add the loss\n",
    "        rec_loss = None\n",
    "        if (self.use_reconst):\n",
    "            x = masked.view (-1,10 * 30)\n",
    "            x = self.fcReconst1(x)\n",
    "            x = self.reluReconst1(x)\n",
    "            x = self.fcReconst2(x)\n",
    "            x = self.reluReconst2(x)\n",
    "            x = self.fcReconst3(x)\n",
    "            x = self.sigmoid(x)\n",
    "            rec_loss = self.reconstruction_loss(x,input_image, size_average)\n",
    "            print(loss, rec_loss)\n",
    "            loss = loss + 0.0005 * rec_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input, dim=1):\n",
    "    \"\"\"\n",
    "    Apply softmax to specific dimensions. Not released on PyTorch stable yet\n",
    "    as of November 6th 2017\n",
    "    https://github.com/pytorch/pytorch/issues/3235\n",
    "    \"\"\"\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(\n",
    "        transposed_input.contiguous().view(-1, transposed_input.size(-1)))\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cn_model= CapsNet(use_reconst= True)\n",
    "if (torch.cuda.is_available()):\n",
    "    cn_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "def decorator(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(self, i):\n",
    "        sample = f(self, i)\n",
    "        if self.conv:\n",
    "            sample['image'] = sample['image'].reshape(1, 96, 96)\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)  \n",
    "        sample['image'] = torch.from_numpy(sample['image'].copy())\n",
    "        if not self.test:\n",
    "            sample['keypoints'] = torch.from_numpy(sample['keypoints'])\n",
    "        return sample\n",
    "    return wrapper\n",
    "\n",
    "def train_valid_split(dataset, test_size = 0.2, shuffle = False, random_seed = 0):\n",
    "    \"\"\" Return a list of splitted indices from a DataSet.\n",
    "    Indices can be used with DataLoader to build a train and validation set.\n",
    "    \n",
    "    Arguments:\n",
    "        A Dataset\n",
    "        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "        Shuffling True or False\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    length = len(dataset)\n",
    "    indices = list(range(1,length))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if type(test_size) is float:\n",
    "        split = int(test_size * length)\n",
    "    elif type(test_size) is int:\n",
    "        split = test_size\n",
    "    else:\n",
    "        raise ValueError('%s should be an int or a float' % str)\n",
    "        \n",
    "    return indices[split:], indices[:split]\n",
    "\n",
    "def load(csvfile, test_split = 0.2, test = False, conv = False, transform = None, cols = None):\n",
    "    \n",
    "    X = FacialKeypointDataset(csvfile, test=test, conv=conv, transform=transform, cols=cols)\n",
    "    \n",
    "    if not test:\n",
    "        # Creating a validation split\n",
    "        if not transform:\n",
    "            train_idx, valid_idx = train_valid_split(X, test_split, shuffle = True)\n",
    "            train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "            valid_sampler = sampler.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "            #print('Train size: ', len(train_sampler), '\\tValidation Size: ', len(valid_sampler))\n",
    "        \n",
    "            # Both dataloader loads from the same dataset but with different indices\n",
    "            train_loader = DataLoader(X,\n",
    "                      batch_size=16,\n",
    "                      sampler=train_sampler,\n",
    "                      num_workers=8)\n",
    "\n",
    "            valid_loader = DataLoader(X,\n",
    "                      batch_size=16,\n",
    "                      sampler=valid_sampler,\n",
    "                      num_workers=8)\n",
    "\n",
    "            dataloaders = {'train':train_loader, 'valid':valid_loader}\n",
    "            dataset_sizes = {'train':len(train_sampler), 'valid':len(valid_sampler)}\n",
    "        \n",
    "        else:\n",
    "            X_valid = FacialKeypointDataset(csvfile, test, conv, transform = None, cols = cols)\n",
    "            train_loader = DataLoader(X,\n",
    "                      batch_size=16,\n",
    "                      num_workers=8)\n",
    "\n",
    "            valid_loader = DataLoader(X_valid,\n",
    "                      batch_size=16,\n",
    "                      num_workers=8)\n",
    "            \n",
    "            dataloaders = {'train':train_loader, 'valid':valid_loader}\n",
    "            dataset_sizes = {'train':len(X), 'valid':len(X_valid)}\n",
    "    else:\n",
    "        batch_size = 16\n",
    "        dataloaders = DataLoader(X, batch_size = batch_size, drop_last = True)\n",
    "        dataset_sizes = {'test' : len(dataloaders) * batch_size}\n",
    "    \n",
    "    return dataloaders, dataset_sizes\n",
    "\n",
    "class FacialKeypointDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, test = False, conv = False, transform = None, cols = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.            \n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.face_keypoint_frame = read_csv(csv_file)\n",
    "        self.cols = cols\n",
    "        \n",
    "        if self.cols:  # get a subset of columns\n",
    "            self.face_keypoint_frame = self.face_keypoint_frame[list(self.cols) + ['Image']]\n",
    "        \n",
    "        self.face_keypoint_frame = self.face_keypoint_frame.dropna() #drop rows with missing entries\n",
    "        self.face_keypoint_frame['Image'] = self.face_keypoint_frame['Image'] \\\n",
    "                        .apply(lambda im: np.fromstring(im, sep = ' ', dtype = np.float32)) \n",
    "        \n",
    "        self.test = test\n",
    "        self.conv = conv\n",
    "        self.transform = transform\n",
    "                \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.face_keypoint_frame)\n",
    "    \n",
    "   \n",
    "    @decorator\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        image = self.face_keypoint_frame.iloc[i, -1] / 255\n",
    "\n",
    "        keypoints = self.face_keypoint_frame.iloc[i, :-1].as_matrix().astype(np.float32)\n",
    "        keypoints = (keypoints - 48) / 48 #normalize between [-1, 1]; given range is [0 95]\n",
    "        sample = {'image': image, 'keypoints': keypoints}\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "dataloaders, dataset_sizes = load('../Dataset/training.csv', test_split = 0.2, conv = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, save_file, scheduler = None):\n",
    "    \n",
    "    since = time.time()\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    num_epochs = model.num_epochs\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for is_train in [True, False]:\n",
    "            model.train(is_train)\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            if (is_train):\n",
    "                phase = \"train\"\n",
    "            else:\n",
    "                phase = \"valid\"\n",
    "            for data in data_loader[phase]:\n",
    "                inputs = data['image']\n",
    "                labels = data['keypoints']\n",
    "                #inputs, labels = Variable(inputs), Variable(labels)\n",
    "                #do not uncomment the four lines below; we will be working with CPU\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                #print(inputs.size())\n",
    "                optimizer.zero_grad()\n",
    "                outputs, s = model(inputs)\n",
    "                \n",
    "                loss = model.model_loss(outputs,s,labels,inputs)\n",
    "                if (is_train):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.data[0]\n",
    "             \n",
    "            epoch_loss = running_loss / num_batches\n",
    "            if (is_train):\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            else:\n",
    "                valid_loss_history.append(epoch_loss)\n",
    "            print('{} Loss: {:.8f}'.format(\n",
    "                    phase, epoch_loss))\n",
    "            if (not is_train):\n",
    "                print('Train Loss / Valid Loss: {:.6f}'.format(\n",
    "                    train_loss_history[-1] / valid_loss_history[-1]))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "    torch.save(model.state_dict(), save_file)\n",
    "    torch.save(train_loss_history, save_file + '_loss_history')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Variable containing:\n",
      " 0.1571\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 497.7518\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1541\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 460.0690\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1396\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 535.0803\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1218\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 523.2101\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1285\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 533.4300\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1267\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 497.5692\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1211\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 514.9830\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1137\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 455.0789\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1121\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 411.0962\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1087\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 485.2431\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1016\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 575.0737\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-1:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5ca09b80e2cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trainedModelCF5'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-13faacead538>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, optimizer, save_file, scheduler)\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cn_model.num_epochs = 1\n",
    "optimizer= torch.optim.Adam(cn_model.parameters(), lr=0.0001)\n",
    "train_model(cn_model, dataloaders, optimizer, 'trainedModelCF5' + str(cn_model.num_epochs) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_keypoints(image, regressed_keypoints, gt_keypoints):\n",
    "    print(gt_keypoints)\n",
    "    \"\"\"Show image with keypoints\"\"\"    \n",
    "    regressed_keypoints = regressed_keypoints * 48 + 48\n",
    "    gt_keypoints = gt_keypoints * 48 + 48\n",
    "    print (regressed_keypoints)\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.scatter(regressed_keypoints[0::2], regressed_keypoints[1::2], marker='.', c='r')\n",
    "    plt.scatter(gt_keypoints[0::2], gt_keypoints[1::2], marker='.', c='g')\n",
    "\n",
    "def predict_plot_conv(model, test_file, model_file, cols = None, num_output_units = 30):\n",
    "                                                    #predict and plot for one batch\n",
    "    test_loader, test_size = load(test_file, test = True, conv = True, cols = cols)\n",
    "    #model.load_state_dict(torch.load(model_file, map_location=lambda storage, loc: storage))\n",
    "    #model.eval()\n",
    "    \n",
    "    batch_size = int(test_size['test'] / len(test_loader))\n",
    "    y_pred = torch.zeros(len(test_loader), batch_size, num_output_units)\n",
    "    \n",
    "    for j, test_data in enumerate(test_loader):\n",
    "        v,s = model(Variable(test_data['image'].cuda()))\n",
    "        _, indexOfMaxV = mask(v)\n",
    "        extracted = extract(s,indexOfMaxV)\n",
    "        y_pred[j] = extracted.view(-1,30,1).data\n",
    "        #y_pred[j] =  torch.norm(model(Variable(test_data['image'].cuda())).data,p=2,dim=2)\n",
    "        \n",
    "        if j == 0:\n",
    "            fig = plt.figure(figsize=(12, 12))\n",
    "            fig.subplots_adjust(\n",
    "                left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                ax = fig.add_subplot(int(np.sqrt(batch_size)) + 1, int(np.sqrt(batch_size)) + 1, \n",
    "                                 i + 1, xticks=[], yticks=[])\n",
    "                show_keypoints(test_data['image'][i].numpy().reshape(96, 96), y_pred[j, i, :].numpy(),\n",
    "                               test_data['keypoints'][i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#testmodel= CapsNet()\n",
    "#if (torch.cuda.is_available()):\n",
    "    #model.cuda()\n",
    "    \n",
    "cn_model.use_reconst = False\n",
    "predict_plot_conv(cn_model,'../Dataset/training.csv','trainedModelCF510.pth')\n",
    "#test_fc_model()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
