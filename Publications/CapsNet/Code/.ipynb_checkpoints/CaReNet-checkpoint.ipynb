{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaReNet - CapsNet based regression network\n",
    "\n",
    "## Facial keypoint Detection using Capsule Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is improvised version of <b>CaFaNet5</b>. <br/>\n",
    "In this network, we activate only one DigitCapsule for a given image. The activated capsule has the most probability and the corresponding non-squashed vector has the required keypoint values. <br/>\n",
    "Masking is used, 10\\*30 non-sqashed output. <br/>\n",
    "Our architecture consists of <br/>\n",
    "i/p -> CNN -> Primary Capsule -> Detection Capsule (10 \\* 30) + retain non-squashed vector ->  Detection Capsule (10 * 30) + retain non-squashed vector -> o/p (30 key points) <br/>\n",
    "error used for back propagation: MSE of non-squashed vector and the ground truths. <br/>\n",
    "<b> Eureka!</b> This model obtains better results compared to a CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from capsnet_utils import *\n",
    "import torch.optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torchvision.utils as vutils\n",
    "from pandas.io.parsers import read_csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_squash(s, dim = 2):\n",
    "    \n",
    "    s_norm = torch.norm(s, p = 2, dim = dim, keepdim = True)\n",
    "    s_norm_sqr = s_norm ** 2     \n",
    "    scalar = s_norm_sqr / (1. + s_norm_sqr)\n",
    "    v = scalar * (s / s_norm)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_activity(W, u):\n",
    "    \n",
    "    num_capsules_higher = W.size(2)\n",
    "    \n",
    "    # size of W: batch_size x # capsules in lower_layer x # capsules in higher_layer \n",
    "    #                       x capsule_dim in higher layer x capsule_dim in lower layer\n",
    "    # size of u: batch_size x # capsules in lower_layer  \n",
    "    #                       x capsule_dim in lower layer x 1\n",
    "    # if we stack u along dim 2 # capsules in higher_layer times, size of u will become\n",
    "    #           batch_size x # capsules in lower_layer x # capsules in higher_layer\n",
    "    #                      x capsule_dim in lower layer x 1\n",
    "    # so we can directly invoke torch.matmul to do the multiplication of two tensors and get u_hat whose size is\n",
    "    #           batch_size x # capsules in lower_layer x # capsules in higher_layer \n",
    "    #                      x capsule_dim in higher layer x 1\n",
    "    \n",
    "    u = torch.stack([u]*num_capsules_higher, dim = 2)\n",
    "    u_hat = torch.matmul(W, u) \n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_routing(u_hat, r):\n",
    "    \"\"\"\n",
    "    Ip:\n",
    "        u_hat: batch_size x #caps in layer l x #caps in layer l+1 x cap_dim in layer l+1 x 1\n",
    "        r: number of routing iterations    \n",
    "    Return:\n",
    "        v: activity vectors in layer l+1.\n",
    "           size of v is batch_size x #caps in layer l+1 x cap_dim in layer l+1 x 1\n",
    "    \"\"\"\n",
    "    batch_size = u_hat.size(0)\n",
    "    b = Variable(torch.zeros(1, u_hat.size(1), u_hat.size(2), 1))\n",
    "    if torch.cuda.is_available():\n",
    "        b = b.cuda()\n",
    "    for riter in range(r):\n",
    "        c = softmax(b, dim = 1) #see eqn(3)\n",
    "        c = torch.cat([c] * batch_size, dim = 0).unsqueeze(4) # to take advantage of python \n",
    "                                                                # broadcasting to do step 5 in Fig 2\n",
    "        s = torch.sum(c * u_hat, dim = 1) # step 5\n",
    "        v = my_squash(s, dim = 2) # step 6\n",
    "        v_temp = torch.stack([v] * u_hat.size(1), dim = 1) # required for vectorizing step 7 in Fig 2\n",
    "        b = b + torch.matmul(u_hat.transpose(3, 4), v_temp).squeeze(4).mean(dim = 0, keepdim = True)\n",
    "                                                           # step 7\n",
    "    return v,s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # create primary capsule layer\n",
    "        #primary capsule has 32 channels of (8D) capsules and each capsule has 8 convolutional units\n",
    "        self.conv_units = nn.ModuleList([nn.Conv2d(256, 32, kernel_size = (16, 16),\n",
    "                                                          stride = (12, 12)) for i in range(8)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # forward prop through primary capsule layer          \n",
    "        outputs = [m(x) for i, m in enumerate(self.conv_units)] # a list of 8 outputs\n",
    "        outputs = torch.stack(outputs, dim = 4) # stack all of them along dim 4\n",
    "        #assert capsules.size() == [x.size(0), 32, 6, 6, 8]\n",
    "        s = outputs.view(x.size(0), -1, 8).unsqueeze(dim = 3) \n",
    "                                            # reshape to size batch_size x 1152 x 8 x 1\n",
    "        v = my_squash(s, 2) # squash along dim 2           \n",
    "        #assert capsules.size() == [x.size(0), 1152, 8, 1]\n",
    "        return v\n",
    "\n",
    "class RoutingCapsuleLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, lowerCapsCount, higherCapsCount, higherCapsDim, lowerCapsDim, iterations):\n",
    "        \n",
    "        super().__init__()\n",
    "        # digits capsule; so create W as a learnable set of parameters initialized randomly\n",
    "        self.W = nn.Parameter(torch.randn(1, lowerCapsCount, higherCapsCount, higherCapsDim, lowerCapsDim)) # 1st dim is 1 since right now we dont\n",
    "                                                               # know batch_size\n",
    "        self.routing_iterations = iterations\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #forward prop through digits capsule by prediciting activity and routing\n",
    "        u_hat = predict_activity(self.W, x)\n",
    "        v, s = dynamic_routing(u_hat, self.routing_iterations)\n",
    "        return v, s   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask(x):\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    #masked_x = Variable(torch.zeros(x.size()))\n",
    "    #if torch.cuda.is_available():\n",
    "     #   masked_x = masked_x.cuda()\n",
    "    v_norm = torch.norm(x, p = 2, dim = 2)   \n",
    "    _, max_index = v_norm.max(dim = 1)\n",
    "    #max_index = max_index.numpy().squeeze().list()\n",
    "    #masked_x[range(batch_size)] = x[range(batch_size), max_index]\n",
    "    max_index = max_index.data    \n",
    "    masked_v = []\n",
    "    max_v_indices = []\n",
    "    for batch_ix in range(batch_size):\n",
    "        # Batch sample\n",
    "        sample = x[batch_ix]\n",
    "        # Masks out the other capsules in this sample.\n",
    "        v = Variable(torch.zeros(sample.size()))\n",
    "        if torch.cuda.is_available():\n",
    "            v = v.cuda()\n",
    "        # Get the maximum capsule index from this batch sample.\n",
    "        max_caps_index = max_index[batch_ix]\n",
    "        v[max_caps_index] = sample[max_caps_index]\n",
    "        max_v_indices.append(max_caps_index)\n",
    "        masked_v.append(v) # append v to masked_v\n",
    "\n",
    "    # Concatenates sequence of masked capsules tensors along the batch dimension.\n",
    "    masked = torch.stack(masked_v, dim=0)\n",
    "    max_i = torch.stack(max_v_indices, dim = 0)\n",
    "    #print(masked.size())\n",
    "    return masked, max_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(s, indices):\n",
    "    batch_size = s.size(0)\n",
    "    max_s_values = []\n",
    "    for batch_ix in range(batch_size):\n",
    "        sample = s[batch_ix]\n",
    "        max_s_values.append(sample[indices[batch_ix]])\n",
    "    extracted = torch.stack(max_s_values, dim = 0)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, use_reconst= True):\n",
    "        \n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(1, 256, kernel_size = (9, 9), stride = (1, 1))        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.primary_capsule = PrimaryCapsuleLayer()\n",
    "        self.detection_capsule1 = RoutingCapsuleLayer(1568, 10, 30, 8, 3)        \n",
    "        self.detection_capsule2 = RoutingCapsuleLayer(10, 10, 30, 30, 3)  \n",
    "        self.use_reconst= use_reconst\n",
    "        \n",
    "        # for margin loss\n",
    "        self.loss_criterion = nn.MSELoss(size_average = True)\n",
    "        \n",
    "        if (self.use_reconst):\n",
    "            self.fcReconst1= nn.Linear(30 * 10,1024)\n",
    "            self.reluReconst1= nn.ReLU()\n",
    "            self.fcReconst2= nn.Linear(1024,4096)\n",
    "            self.reluReconst2= nn.ReLU()\n",
    "            self.fcReconst3= nn.Linear(4096, 9216)\n",
    "            self.sigmoid= nn.Sigmoid()\n",
    "               \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)        \n",
    "        x = self.relu1(x)\n",
    "        x = self.primary_capsule(x)        \n",
    "        x, _ = self.detection_capsule1(x) \n",
    "        #Here's our innovation; return squashed and non-sqauashed values\n",
    "        x, s = self.detection_capsule2(x)\n",
    "        return x,s\n",
    "    \n",
    "    def margin_loss(self, model_output, ground_truth):\n",
    "        model_output = model_output.view(-1,30,1)\n",
    "        return self.loss_criterion(model_output, ground_truth)\n",
    "        \n",
    "    def reconstruction_loss(self, reconstructed, original, size_average= True):\n",
    "        #square\n",
    "        rec_loss = (reconstructed - original.view(reconstructed.size(0), -1)) ** 2\n",
    "        #sum\n",
    "        rec_loss = torch.sum(rec_loss, dim = 1)\n",
    "        #mean\n",
    "        if (size_average):\n",
    "            rec_loss= rec_loss.mean()\n",
    "        return rec_loss\n",
    "    \n",
    "    def model_loss(self, x,s, ground_truth, input_image, size_average= True):\n",
    "        \n",
    "        #output_norm = torch.norm(model_output, p = 2, dim = 2) \n",
    "        masked, indices = mask(x)\n",
    "        extracted = extract(s, indices)\n",
    "        # calculate margin loss\n",
    "        loss = self.margin_loss(extracted, ground_truth)\n",
    "        # if use reconstruction, then reconstruct and add the loss\n",
    "        rec_loss = None\n",
    "        if (self.use_reconst):\n",
    "            x = masked.view (-1,10 * 30)\n",
    "            x = self.fcReconst1(x)\n",
    "            x = self.reluReconst1(x)\n",
    "            x = self.fcReconst2(x)\n",
    "            x = self.reluReconst2(x)\n",
    "            x = self.fcReconst3(x)\n",
    "            x = self.sigmoid(x)\n",
    "            rec_loss = self.reconstruction_loss(x,input_image, size_average)\n",
    "            print(loss, rec_loss)\n",
    "            loss = loss + 0.0005 * rec_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input, dim=1):\n",
    "    \"\"\"\n",
    "    Apply softmax to specific dimensions. Not released on PyTorch stable yet\n",
    "    as of November 6th 2017\n",
    "    https://github.com/pytorch/pytorch/issues/3235\n",
    "    \"\"\"\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(\n",
    "        transposed_input.contiguous().view(-1, transposed_input.size(-1)))\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cn_model= CapsNet(use_reconst= True)\n",
    "if (torch.cuda.is_available()):\n",
    "    cn_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "def decorator(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(self, i):\n",
    "        sample = f(self, i)\n",
    "        if self.conv:\n",
    "            sample['image'] = sample['image'].reshape(1, 96, 96)\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)  \n",
    "        sample['image'] = torch.from_numpy(sample['image'].copy())\n",
    "        if not self.test:\n",
    "            sample['keypoints'] = torch.from_numpy(sample['keypoints'])\n",
    "        return sample\n",
    "    return wrapper\n",
    "\n",
    "def train_valid_split(dataset, test_size = 0.2, shuffle = False, random_seed = 0):\n",
    "    \"\"\" Return a list of splitted indices from a DataSet.\n",
    "    Indices can be used with DataLoader to build a train and validation set.\n",
    "    \n",
    "    Arguments:\n",
    "        A Dataset\n",
    "        A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "        Shuffling True or False\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    length = len(dataset)\n",
    "    indices = list(range(1,length))\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if type(test_size) is float:\n",
    "        split = int(test_size * length)\n",
    "    elif type(test_size) is int:\n",
    "        split = test_size\n",
    "    else:\n",
    "        raise ValueError('%s should be an int or a float' % str)\n",
    "        \n",
    "    return indices[split:], indices[:split]\n",
    "\n",
    "def load(csvfile, test_split = 0.2, test = False, conv = False, transform = None, cols = None):\n",
    "    \n",
    "    X = FacialKeypointDataset(csvfile, test=test, conv=conv, transform=transform, cols=cols)\n",
    "    \n",
    "    if not test:\n",
    "        # Creating a validation split\n",
    "        if not transform:\n",
    "            train_idx, valid_idx = train_valid_split(X, test_split, shuffle = True)\n",
    "            train_sampler = sampler.SubsetRandomSampler(train_idx)\n",
    "            valid_sampler = sampler.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "            #print('Train size: ', len(train_sampler), '\\tValidation Size: ', len(valid_sampler))\n",
    "        \n",
    "            # Both dataloader loads from the same dataset but with different indices\n",
    "            train_loader = DataLoader(X,\n",
    "                      batch_size=16,\n",
    "                      sampler=train_sampler,\n",
    "                      num_workers=8)\n",
    "\n",
    "            valid_loader = DataLoader(X,\n",
    "                      batch_size=16,\n",
    "                      sampler=valid_sampler,\n",
    "                      num_workers=8)\n",
    "\n",
    "            dataloaders = {'train':train_loader, 'valid':valid_loader}\n",
    "            dataset_sizes = {'train':len(train_sampler), 'valid':len(valid_sampler)}\n",
    "        \n",
    "        else:\n",
    "            X_valid = FacialKeypointDataset(csvfile, test, conv, transform = None, cols = cols)\n",
    "            train_loader = DataLoader(X,\n",
    "                      batch_size=16,\n",
    "                      num_workers=8)\n",
    "\n",
    "            valid_loader = DataLoader(X_valid,\n",
    "                      batch_size=16,\n",
    "                      num_workers=8)\n",
    "            \n",
    "            dataloaders = {'train':train_loader, 'valid':valid_loader}\n",
    "            dataset_sizes = {'train':len(X), 'valid':len(X_valid)}\n",
    "    else:\n",
    "        batch_size = 16\n",
    "        dataloaders = DataLoader(X, batch_size = batch_size, drop_last = True)\n",
    "        dataset_sizes = {'test' : len(dataloaders) * batch_size}\n",
    "    \n",
    "    return dataloaders, dataset_sizes\n",
    "\n",
    "class FacialKeypointDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, test = False, conv = False, transform = None, cols = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.            \n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.face_keypoint_frame = read_csv(csv_file)\n",
    "        self.cols = cols\n",
    "        \n",
    "        if self.cols:  # get a subset of columns\n",
    "            self.face_keypoint_frame = self.face_keypoint_frame[list(self.cols) + ['Image']]\n",
    "        \n",
    "        self.face_keypoint_frame = self.face_keypoint_frame.dropna() #drop rows with missing entries\n",
    "        self.face_keypoint_frame['Image'] = self.face_keypoint_frame['Image'] \\\n",
    "                        .apply(lambda im: np.fromstring(im, sep = ' ', dtype = np.float32)) \n",
    "        \n",
    "        self.test = test\n",
    "        self.conv = conv\n",
    "        self.transform = transform\n",
    "                \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.face_keypoint_frame)\n",
    "    \n",
    "   \n",
    "    @decorator\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        image = self.face_keypoint_frame.iloc[i, -1] / 255\n",
    "\n",
    "        keypoints = self.face_keypoint_frame.iloc[i, :-1].as_matrix().astype(np.float32)\n",
    "        keypoints = (keypoints - 48) / 48 #normalize between [-1, 1]; given range is [0 95]\n",
    "        sample = {'image': image, 'keypoints': keypoints}\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "dataloaders, dataset_sizes = load('./data/training.csv', test_split = 0.2, conv = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, save_file, scheduler = None):\n",
    "    \n",
    "    since = time.time()\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    num_epochs = model.num_epochs\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        for is_train in [True, False]:\n",
    "            model.train(is_train)\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            if (is_train):\n",
    "                phase = \"train\"\n",
    "            else:\n",
    "                phase = \"valid\"\n",
    "            for data in data_loader[phase]:\n",
    "                inputs = data['image']\n",
    "                labels = data['keypoints']\n",
    "                #inputs, labels = Variable(inputs), Variable(labels)\n",
    "                #do not uncomment the four lines below; we will be working with CPU\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                #print(inputs.size())\n",
    "                optimizer.zero_grad()\n",
    "                outputs, s = model(inputs)\n",
    "                \n",
    "                loss = model.model_loss(outputs,s,labels,inputs)\n",
    "                if (is_train):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.data[0]\n",
    "             \n",
    "            epoch_loss = running_loss / num_batches\n",
    "            if (is_train):\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            else:\n",
    "                valid_loss_history.append(epoch_loss)\n",
    "            print('{} Loss: {:.8f}'.format(\n",
    "                    phase, epoch_loss))\n",
    "            if (not is_train):\n",
    "                print('Train Loss / Valid Loss: {:.6f}'.format(\n",
    "                    train_loss_history[-1] / valid_loss_history[-1]))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "    torch.save(model.state_dict(), save_file)\n",
    "    torch.save(train_loss_history, save_file + '_loss_history')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n",
      "Variable containing:\n",
      " 0.1571\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 497.7518\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1541\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 460.0690\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1396\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 535.0803\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1218\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 523.2101\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1285\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 533.4300\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1267\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 497.5692\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1211\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 514.9830\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1137\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 455.0789\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1121\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 411.0962\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1087\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 485.2431\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.1016\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      " Variable containing:\n",
      " 575.0737\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-2:\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-1:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/anaconda3/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5ca09b80e2cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trainedModelCF5'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-13faacead538>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, optimizer, save_file, scheduler)\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cn_model.num_epochs = 1\n",
    "optimizer= torch.optim.Adam(cn_model.parameters(), lr=0.0001)\n",
    "train_model(cn_model, dataloaders, optimizer, 'trainedModelCF5' + str(cn_model.num_epochs) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_keypoints(image, regressed_keypoints, gt_keypoints):\n",
    "    print(gt_keypoints)\n",
    "    \"\"\"Show image with keypoints\"\"\"    \n",
    "    regressed_keypoints = regressed_keypoints * 48 + 48\n",
    "    gt_keypoints = gt_keypoints * 48 + 48\n",
    "    print (regressed_keypoints)\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.scatter(regressed_keypoints[0::2], regressed_keypoints[1::2], marker='.', c='r')\n",
    "    plt.scatter(gt_keypoints[0::2], gt_keypoints[1::2], marker='.', c='g')\n",
    "\n",
    "def predict_plot_conv(model, test_file, model_file, cols = None, num_output_units = 30):\n",
    "                                                    #predict and plot for one batch\n",
    "    test_loader, test_size = load(test_file, test = True, conv = True, cols = cols)\n",
    "    #model.load_state_dict(torch.load(model_file, map_location=lambda storage, loc: storage))\n",
    "    #model.eval()\n",
    "    \n",
    "    batch_size = int(test_size['test'] / len(test_loader))\n",
    "    y_pred = torch.zeros(len(test_loader), batch_size, num_output_units)\n",
    "    \n",
    "    for j, test_data in enumerate(test_loader):\n",
    "        v,s = model(Variable(test_data['image'].cuda()))\n",
    "        _, indexOfMaxV = mask(v)\n",
    "        extracted = extract(s,indexOfMaxV)\n",
    "        y_pred[j] = extracted.view(-1,30,1).data\n",
    "        #y_pred[j] =  torch.norm(model(Variable(test_data['image'].cuda())).data,p=2,dim=2)\n",
    "        \n",
    "        if j == 0:\n",
    "            fig = plt.figure(figsize=(12, 12))\n",
    "            fig.subplots_adjust(\n",
    "                left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                ax = fig.add_subplot(int(np.sqrt(batch_size)) + 1, int(np.sqrt(batch_size)) + 1, \n",
    "                                 i + 1, xticks=[], yticks=[])\n",
    "                show_keypoints(test_data['image'][i].numpy().reshape(96, 96), y_pred[j, i, :].numpy(),\n",
    "                               test_data['keypoints'][i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37569919 -0.18745263 -0.3702707  -0.24121507  0.24129327 -0.17401202\n",
      "  0.5235489  -0.16729172 -0.24257143 -0.22105415 -0.51139849 -0.22105415\n",
      "  0.1865263  -0.39513233  0.67139846 -0.32858047 -0.16192484 -0.39578497\n",
      " -0.65924209 -0.38234437 -0.07457145  0.18889172  0.27490225  0.66604519\n",
      " -0.40386465  0.61227065 -0.0976541   0.51948863 -0.10144361  0.76012021]\n",
      "[ 47.99871445  48.00097656  47.99902725  48.00688934  47.99939346\n",
      "  47.99996185  48.00052643  47.99826431  48.00147247  48.00225449\n",
      "  48.00086212  48.00072479  48.00097656  48.00324249  47.99938202\n",
      "  48.00431061  47.99719238  47.99950409  48.00178909  47.99841309\n",
      "  47.99942017  47.99859619  48.0001564   47.99928665  47.99992371\n",
      "  48.00421524  48.00410461  48.00152969  48.00305176  47.99980164]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503963423183/work/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-39fbf1a83ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_reconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpredict_plot_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcn_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data/training.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'trainedModelCF510.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#test_fc_model()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-9cc24afda922>\u001b[0m in \u001b[0;36mpredict_plot_conv\u001b[1;34m(model, test_file, model_file, cols, num_output_units)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexOfMaxV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mextracted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindexOfMaxV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-651682afd137>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_capsule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetection_capsule1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda3/lib/python3.5/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[1;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mThreshold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, input, *params)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503963423183/work/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALQAAAC0CAYAAAA9zQYyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/KXKO6Dfy1+qvAP6vfylyjug38tfqvwfH0fcC50zn5MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f11980a0390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#testmodel= CapsNet()\n",
    "#if (torch.cuda.is_available()):\n",
    "    #model.cuda()\n",
    "    \n",
    "cn_model.use_reconst = False\n",
    "predict_plot_conv(cn_model,'data/training.csv','trainedModelCF510.pth')\n",
    "#test_fc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
